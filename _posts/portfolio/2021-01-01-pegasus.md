---
layout: default
title: A Framework for Scientific Article Summarization
category: portfolio
modal-id: 7
img: 2pegasus.png
alt: Scientific Article Summarization
client: Natural Language Processing
application: Scientific Article Summarization
project-date: 2021
languages:
- Python
concepts:
- Natural Language Processing
tools:
- PEGASUS
stack:
- Jupyter Notebooks
---
### Project Links

Paper Link: https://drive.google.com/file/d/1RPgxXayLxy9lk032AnX1IgYuxN6uzqbr/view?usp=drive_link

Presentation Link: https://drive.google.com/file/d/1ConyUErEyBwBXLmXT6xw_Pjk7GN9JiTX/view?usp=drive_link

### Project Description

This scientific article proposes a framework for improving the abstractive summarization of scientific papers. The authors focus on enhancing state-of-the-art transformer techniques, such as PEGASUS. They introduce a custom pre-processing scoring technique to reduce the number of sentences that the transformer is fed, based on sentence quality metrics. The results show improved performance in terms of ROUGE-N and ROUGE-L scores for all model types, indicating the effectiveness of their scoring method. The authors also explore a custom attention mechanism for selecting top words, an ensemble Pegasus approach, and fine-tuning a Pegasus transformer on novel scientific data.

The introduction highlights the importance of literature review in research projects and the benefits of automatically summarizing scientific articles. The authors distinguish between abstractive and extractive summarization approaches, with a focus on the former. They discuss the differences between summarizing generic texts and scientific articles, particularly in terms of structure and length.

The background section provides an overview of previous research in scientific summarization, including approaches based on abstract generation, citation-based summarization, and abstractive/extractive techniques. The authors mention the limitations of existing projects due to the lack of available papers for analysis and training. They introduce the PEGASUS model as the baseline model for their experiments, highlighting its self-supervised pre-training objective.

The methods section describes the datasets used in the study, including ScisummNet, Arxiv, and AAN. The authors explain the steps involved in preparing the ScisummNet dataset for training PEGASUS, such as extracting cited text spans and evaluating sentence quality. They also outline the different exercises conducted, including applying PEGASUS to the ScisummNet data, fine-tuning PEGASUS with ScisummNet data, and exploring a custom attention mechanism and ensemble approach.

The experimentation section presents the results of the exercises. It includes the evaluation of PEGASUS performance on the ScisummNet dataset, the fine-tuning of PEGASUS with ScisummNet data, the impact of the "Body Quality" metric on PEGASUS performance, the analysis of attention mechanisms in the PEGASUS model, and the effectiveness of the ensemble approach. The authors compare the achieved results with previous approaches and demonstrate marginal improvements in ROUGE scores.

Overall, the framework proposed in this article aims to enhance the abstractive summarization of scientific papers by incorporating custom pre-processing techniques, attention mechanisms, and ensemble approaches. The results suggest that these techniques can improve the performance of existing transformer models and contribute to faster literature review processes.
